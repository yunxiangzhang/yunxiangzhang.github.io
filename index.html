<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yunxiang Zhang</title>

  <meta name="author" content="Yunxiang Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="images/Icon.jpg">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Yunxiang Zhang</name>
                  </p>
                  <p>
                    Welcome! I am a Computer Science Ph.D. student at <a href="https://www.nyu.edu">NYU</a>, advised by <a href="https://qisun.me">Prof. Qi Sun</a> in the <a href="https://www.immersivecomputinglab.org">Immersive
                      Computing Lab</a>. My current research revolves around virtual/augmented/mixed reality, human-computer interaction, perceptual computer graphics, and generative AI, with a particular focus on multimodal interface design and AI-assisted content creation that bridge the gap between immersive virtual experience and real-world physicality. More broadly, I enjoy combining theoretical insights from physical, perceptual, and cognitive sciences with machine learning tools to solve challenging real-world problems.
                  </p>
                  <p>
                    Prior to NYU, I obtained my M.Phil in the <a href="https://mmlab.ie.cuhk.edu.hk">Multimedia Laboratory</a> at <a href="https://www.gs.cuhk.edu.hk">CUHK</a> under the supervision of <a
                      href="http://dahua.site">Prof. Dahua Lin</a>. Before that, I did my
                    undergraduate studies at <a href="https://en.sjtu.edu.cn">SJTU</a> and <a href="https://www.polytechnique.edu/en">École Polytechnique</a>.
                  </p>
                  <P>
                    During my graduate studies, I have had the fortune to work with and learn from great minds at <a href="https://www.intel.com/content/www/us/en/developer/topic-technology/graphics-research/overview.html">Intel Graphics Research</a>, <a href="https://vectorinstitute.ai">Vector Institute</a>, and <a href="https://www.telecom-paris.fr/en/home">Télécom Paris</a> through internships.
                  <p>
                    Outside of work, I love hiking, traveling, and photography!
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:yunxiang.zhang@nyu.edu">Email</a> &nbsp/&nbsp
                    <a href="files/Yunxiang.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=ODbxhqsAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://twitter.com/YunxiangSJTU">Twitter</a> &nbsp/&nbsp
                    <a href="https://github.com/yunxiangzhang">GitHub</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/Yunxiang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Yunxiang.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
            <tbody>
              <tr>
                <td>
                  <heading>Updates</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="10">
            <tr>
              <td>
                <li>
                  <strong>Aug 2024: </strong> Our work on saliency-guided image generation got accepted to TAP and won <span style="color: #e0144c;"> <i class="fas fa-award"></i> Best Paper Award</span>, <span style="color: #e0144c;"> <i class="fas fa-award"></i> Best Presentation Award</span> at SAP 2024!
                </li>
                <br>
                <li>
                  <strong>Jul 2024: </strong> Two papers, one on VR force interface and one on interative virtual agent got accepted to ISMAR 2024, see you in Seattle!
                </li>
                <br>
                <li>
                  <strong>Jun 2024: </strong> Our work on visual-auditory integration got accepted to TVCG!
                </li>
                <br>
                <li>
                  <strong>May 2024: </strong> Our work on saccadic response acceleration got accepted to SIGGRAPH 2024, see you in Denver!
                </li>
                <br>
                <li>
                  <strong>Apr 2024: </strong> I received the 2024 Deborah Rosenthal MD Award at NYU Tandon.
                </li>
                <!-- <br>
                <li>
                  <strong>May 2023: </strong> I started an internship at <a href="https://www.intel.com/content/www/us/en/developer/topic-technology/graphics-research/overview.html">Intel Graphics Research</a>, mentored by <a href="https://www.alexku.me">Dr. Alexandr Kuznetsov</a> and <a href="https://akshayjindal.com">Dr. Akshay Jindal</a>.
                </li>
                <br>
                <li>
                  <strong>Apr 2023: </strong> Our work on optimized VR/AR ergonomics got accepted to SIGGRAPH 2023, see you in LA!
                </li> -->
              </td>
            </tr>
          </table>
          <br>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
            <tbody>
              <tr>
                <td>
                  <heading>Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/saliency-guided-image-generation.gif" alt="fast-texture" width="240" height="254">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.immersivecomputinglab.org/publication/gazefusion-saliency-guided-image-generation/"></a>
                  <papertitle>GazeFusion: Saliency-guided Image Generation</papertitle>
                  </a>
                  <br>
                  <strong>Yunxiang Zhang</strong>,
                  Nan Wu,
                  <a href="https://connorzlin.com/">Connor Lin</a>,
                  <a href="https://web.stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
                  <a href="https://qisun.me">Qi Sun</a>
                  <br>
                  ACM Transactions on Applied Perception (ACM SAP 2024)
                  <br>
                  <span style="color: #e0144c;"> <i class="fas fa-award"></i> Best Paper Award</span> &nbsp; <span style="color: #e0144c;"> <i class="fas fa-award"></i> Best Presentation Award</span>
                  <br>
                  <a href="https://www.immersivecomputinglab.org/wp-content/uploads/2024/09/gaze_fusion_tap.pdf">Paper</a> /
                  <a href="https://www.immersivecomputinglab.org/publication/gazefusion-saliency-guided-image-generation/">Project Page</a> /
                  <a href="https://www.youtube.com/watch?v=vFa8cyYhdD4&t=3s">Video</a> /
                  <a href="https://github.com/NYU-ICL/saliency-guided-image-generation">Code</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/visual-auditory-integration.jpg" alt="fast-texture" width="240" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.immersivecomputinglab.org/publication/measuring-and-predicting-multisensory-reaction-latency-a-probabilistic-model-for-visual-auditory-integration/"></a>
                  <papertitle>Measuring and Predicting Multisensory Reaction Latency: A Probabilistic Model for Visual-Auditory Integration</papertitle>
                  </a>
                  <br>
                  Xi Peng,
                  <strong>Yunxiang Zhang</strong>,
                  <a href="https://people.mpi-inf.mpg.de/~djimenez/">Daniel Jiménez Navarro</a>,
                  <a href="https://ana-serrano.github.io/">Ana Serrano</a>,
                  <a href="https://people.mpi-inf.mpg.de/~karol/">Karol Myszkowski</a>,
                  <a href="https://qisun.me">Qi Sun</a>
                  <br>
                  IEEE Transactions on Visualization and Computer Graphics (IEEE TVCG 2024)
                  <br>
                  <a href="https://www.computer.org/csdl/journal/tg/5555/01/10670078/206qqO9oW08">Paper</a> /
                  <a href="https://www.immersivecomputinglab.org/publication/measuring-and-predicting-multisensory-reaction-latency-a-probabilistic-model-for-visual-auditory-integration/">Project Page</a> /
                  <a href="https://www.youtube.com/watch?v=nSEzDk0gIAk">Video</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/improved-xr-force-interface.jpg" alt="fast-texture" width="240" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.immersivecomputinglab.org/publication/may-the-force-be-with-you-dexterous-finger-force-aware-xr-interface/"></a>
                  <papertitle>May the Force Be with You: Dexterous Finger Force-Aware VR Interface</papertitle>
                  </a>
                  <br>
                  Fengze Zhang*,
                  <strong>Yunxiang Zhang*</strong>,
                  Xi Peng,
                  Sky Achitoff,
                  <a href="https://engineering.nyu.edu/faculty/paul-torrens">Paul M. Torrens</a>,
                  <a href="https://qisun.me">Qi Sun</a>
                  <br>
                  IEEE International Symposium on Mixed and Augmented Reality (IEEE ISMAR 2024)
                  <br>
                  <a href="https://www.immersivecomputinglab.org/wp-content/uploads/2024/09/emg_force_ui.pdf">Paper</a> /
                  <a href="https://www.immersivecomputinglab.org/publication/may-the-force-be-with-you-dexterous-finger-force-aware-xr-interface/">Project Page</a> /
                  <a href="https://github.com/NYU-ICL/vr-force-aware-multimodal-interface">Code</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/interactive-virtual-agent.jpg" alt="fast-texture" width="240" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.immersivecomputinglab.org/research/"></a>
                  <papertitle>Toward User-Aware Interactive Virtual Agents: Generative Multi-Modal Avatar Behaviors in VR</papertitle>
                  </a>
                  <br>
                  Bhasura Gunawardhana,
                  <strong>Yunxiang Zhang</strong>,
                  <a href="https://qisun.me">Qi Sun</a>,
                  <a href="https://graphics.cs.uh.edu/zdeng/">Zhigang Deng</a>
                  <br>
                  IEEE International Symposium on Mixed and Augmented Reality (IEEE ISMAR 2024)
                  <br>
                  <a href="https://www.immersivecomputinglab.org/wp-content/uploads/2024/08/2024-ISMAR-AgentBehaviorGeneration.pdf">Paper</a> /
                  <a href="https://www.immersivecomputinglab.org/research/">Project Page</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/saccadic-response-acceleration.jpg" alt="fast-texture" width="240" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://avsaccadeaccel.mpi-inf.mpg.de/"></a>
                  <papertitle>Accelerating Saccadic Response through Spatial and Temporal Cross-Modal Misalignments</papertitle>
                  </a>
                  <br>
                  <a href="https://people.mpi-inf.mpg.de/~djimenez/">Daniel Jiménez Navarro</a>,
                  Xi Peng,
                  <strong>Yunxiang Zhang</strong>,
                  <a href="https://people.mpi-inf.mpg.de/~karol/">Karol Myszkowski</a>,
                  <a href="https://people.mpi-inf.mpg.de/~hpseidel/">Hans-Peter Seidel</a>,
                  <a href="https://qisun.me">Qi Sun</a>,
                  <a href="https://ana-serrano.github.io/">Ana Serrano</a>
                  <br>
                  ACM SIGGRAPH 2024
                  <br>
                  <a href="https://avsaccadeaccel.mpi-inf.mpg.de/resource/paper.pdf">Paper</a> /
                  <a href="https://avsaccadeaccel.mpi-inf.mpg.de/">Project Page</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/xr-ergonomics-neck-comfort.jpg" alt="fast-texture" width="240" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.immersivecomputinglab.org/publication/toward-optimized-vr-ar-ergonomics-modeling-and-predicting-user-neck-muscle-contraction/"></a>
                  <papertitle>Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck Muscle Contraction</papertitle>
                  </a>
                  <br>
                  <strong>Yunxiang Zhang</strong>,
                  <a href="https://kenchen10.github.io">Kenneth Chen</a>,
                  <a href="https://qisun.me">Qi Sun</a>
                  <br>
                  ACM SIGGRAPH 2023
                  <br>
                  <a href="https://www.immersivecomputinglab.org/wp-content/uploads/2023/05/xr-ergonomics-neck-comfort.pdf">Paper</a> /
                  <a href="https://www.immersivecomputinglab.org/publication/toward-optimized-vr-ar-ergonomics-modeling-and-predicting-user-neck-muscle-contraction/">Project Page</a> /
                  <a href="https://www.youtube.com/watch?v=XO8VR1tJoaI">Video</a> /
                  <a href="https://github.com/NYU-ICL/xr-ergonomics-neck-comfort">Code</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/xr-emg-force-interface.jpg" alt="fast-texture" width="240" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.immersivecomputinglab.org/publication/force-aware-interface-via-electromyography-for-natural-vr-ar-interaction/">
                    <papertitle>Force-Aware Interface via Electromyography for Natural VR/AR Interaction</papertitle>
                  </a>
                  <br>
                  <strong>Yunxiang Zhang</strong>,
                  Benjamin Liang,
                  Boyuan Chen,
                  <a href="https://engineering.nyu.edu/faculty/paul-torrens">Paul M. Torrens</a>,
                  <a href="https://wp.nyu.edu/meriit/team/">S. Farokh Atashzar</a>,
                  <a href="http://dahua.site">Dahua Lin</a>,
                  <a href="https://qisun.me">Qi Sun</a>
                  <br>
                  ACM Transactions on Graphics (ACM SIGGRAPH Asia 2022)
                  <br>
                  <a href="https://www.immersivecomputinglab.org/wp-content/uploads/2022/10/xr-emg-force-interface.pdf">Paper</a> /
                  <a href="https://www.immersivecomputinglab.org/publication/force-aware-interface-via-electromyography-for-natural-vr-ar-interaction/">Project Page</a> /
                  <a href="https://www.youtube.com/watch?v=Y95LgJT0-Ks">Video</a> /
                  <a href="https://github.com/NYU-ICL/xr-emg-force-interface">Code</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/similarity-based-pruning.jpg" alt="fast-texture" width="256" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ieeexplore.ieee.org/document/10052713">
                    <papertitle>Exploiting Channel Similarity for Network Pruning</papertitle>
                  </a>
                  <br>
                  Chenglong Zhao,
                  <strong>Yunxiang Zhang</strong>,
                  Bingbing Ni
                  <br>
                  IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT 2023)
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/10052713">Paper</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/capc-learning.jpg" alt="fast-texture" width="300" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="http://www.cleverhans.io/2021/05/01/capc.html">
                    <papertitle>CaPC Learning: Confidential and Private Collaborative Learning</papertitle>
                  </a>
                  <br>
                  <a href="https://www.christopherchoquette.com">Christopher A. Choquette-Choo*</a>,
                  <a href="https://ndullerud.github.io">Natalie Dullerud*</a>,
                  <a href="https://adam-dziedzic.com">Adam Dziedzic*</a>,
                  <strong>Yunxiang Zhang*</strong>,
                  <a href="https://pages.cs.wisc.edu/~jha/">Somesh Jha</a>,
                  <a href="https://www.papernot.fr">Nicolas Papernot</a>,
                  <a href="https://wangxiao1254.github.io">Xiao Wang</a>
                  <br>
                  International Conference on Learning Representations (ICLR 2021)
                  <br>
                  <a href="https://openreview.net/pdf?id=h2EbJ4_wMVq">Paper</a> /
                  <a href="http://www.cleverhans.io/2021/05/01/capc.html">Project Page</a> /
                  <a href="https://www.youtube.com/watch?v=3Fp9NpVIwLE">Video</a> /
                  <a href="https://github.com/cleverhans-lab/capc-iclr">Code</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/maxplus-network.jpg" alt="fast-texture" width="236" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://perso.telecom-paristech.fr/bloch/papers/proceedings/ISMM2019-Yunxiang.pdf">
                    <papertitle>Max-plus Operators Applied to Filter Selection and Model Pruning in Neural Networks</papertitle>
                  </a>
                  <br>
                  <strong>Yunxiang Zhang</strong>,
                  <a href="https://samyblusseau.jimdofree.com">Samy Blusseau</a>,
                  <a href="https://people.cmm.minesparis.psl.eu/users/velasco/">Santiago Velasco-Forero</a>,
                  <a href="https://perso.telecom-paristech.fr/bloch/">Isabelle Bloch</a>,
                  <a href="https://people.cmm.minesparis.psl.eu/users/angulo/">Jesus Angulo</a>
                  <br>
                  International Symposium on Mathematical Morphology and Its Application to Signal and Image Processing (ISMM 2019)
                  <br>
                  <a href="https://perso.telecom-paristech.fr/bloch/papers/proceedings/ISMM2019-Yunxiang.pdf">Paper</a> /
                  <a href="https://github.com/yunxiangzhang/Maxplus-Operators">Code</a>
                </td>
              </tr>

              <br>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"></td>
                <td width="75%" valign="middle">* Equal contributions, authors ordered alphabetically</td>
              </tr>
            </tbody>
          </table>
          <br>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
            <tbody>
              <tr>
                <td>
                  <heading>Work Experience</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="10">
            <tr>
              <td>
                Research Intern at <a href="https://www.intel.com/content/www/us/en/developer/topic-technology/graphics-research/overview.html">Intel Graphics Research</a>
                <span style="float:right;">May 2023 - Aug 2023</span>
                <br>
                <br>
                Research Intern at <a href="https://vectorinstitute.ai">Vector Institute</a>
                <span style="float:right;">Mar 2020 - Jun 2020</span>
                <br>
                <br>
                Research Intern at <a href="https://www.telecom-paris.fr/en/home">Télécom Paris</a>
                <span style="float:right;">Apr 2018 - Aug 2018 </span>
              </td>
            </tr>
          </table>
          <br>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
            <tbody>
              <tr>
                <td>
                  <heading>Academic Services</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="10">
            <tr>
              <td>
                <strong>Conference Reviewer:</strong> SIGGRAPH, SIGGRAPH Asia, TVCG, AAAI, IEEE VR, ISMAR, PG
              </td>
            </tr>
          </table>
          <br>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
            <tbody>
              <tr>
                <td style="padding:0px;width:23%;vertical-align:left">
                  <br>
                  <div style="width: 50%; text-align: center; margin: 0 auto;">
                    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=b15gK96yi-6692E5CIh24VZ7u9heqyZKtvpSuUeVPF8"></script>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Template borrowed from <a style="font-size:small;" href="https://jonbarron.info/">Jon Barron</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


        </td>
      </tr>
  </table>
  </td>
  </tr>
  </table>
</body>

</html>